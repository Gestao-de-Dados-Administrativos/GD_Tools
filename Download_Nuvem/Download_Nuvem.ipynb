{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download de Dados da Nuvem CAED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import openpyxl\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, VARCHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURAÇÕES ====================\n",
    "\n",
    "# API\n",
    "URL_BASE = 'https://parc.caeddigital.net/portal/classes'\n",
    "HEADERS = {\n",
    "    'X-Parse-Application-Id': 'portal',\n",
    "    'X-Parse-Client-Key': 'js3.4.2',\n",
    "    'X-Parse-Master-Key': '(daKjG_?x_xvba9',\n",
    "    'X-Parse-Installation-Id': 'd35b5929-fcc4-45f4-8bed-44f219c2d533',\n",
    "    'Content-Type': 'application/json',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "\n",
    "# Banco de Dados\n",
    "SERVIDOR = '192.168.250.8,61433'\n",
    "BANCO_DE_DADOS = 'SDV_TESTE_IMPORTACAO'\n",
    "TRUSTED_CONNECTION = 'yes'\n",
    "\n",
    "# Processamento\n",
    "BATCH_SIZE = 10000\n",
    "COLLECTIONS = ['E_1308_ESTADO']\n",
    "\n",
    "# Configuração do Where\n",
    "WHERE_CLAUSE = {\"excluido\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download dos Dados da API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_todos_dados_api(collection, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Baixa todos os dados de uma collection da API.\n",
    "    \"\"\"\n",
    "    url = f\"{URL_BASE}/{collection}\"\n",
    "    \n",
    "    all_results = []\n",
    "    skip = 0\n",
    "    total_retrieved = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            payload = json.dumps({\n",
    "                \"where\": WHERE_CLAUSE,\n",
    "                \"limit\": batch_size,\n",
    "                \"skip\": skip\n",
    "            })\n",
    "            \n",
    "            response = requests.get(url, headers=HEADERS, data=payload)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            results = data.get('results', [])\n",
    "            \n",
    "            if not results:\n",
    "                break\n",
    "                \n",
    "            all_results.extend(results)\n",
    "            total_retrieved += len(results)\n",
    "            skip += batch_size\n",
    "            \n",
    "            print(f\"Collection {collection}: Baixados {total_retrieved} registros...\\n\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao acessar a API para collection {collection}: {e}\")\n",
    "    \n",
    "    return pd.json_normalize(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar dados das collections\n",
    "dataframes = {col: obter_todos_dados_api(col) for col in COLLECTIONS}\n",
    "\n",
    "# # Criar variável de atalho para a collection principal\n",
    "# enturmacao = dataframes['E_1308_ESTADO']\n",
    "\n",
    "# # Mostrar informações\n",
    "# display(enturmacao.head())\n",
    "# print(f\"\\nTotal de registros: {len(enturmacao)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparação para Importação no SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar engine de conexão\n",
    "string_conexao = f'mssql+pyodbc://{SERVIDOR}/{BANCO_DE_DADOS}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection={TRUSTED_CONNECTION}'\n",
    "engine = create_engine(string_conexao, fast_executemany=True)\n",
    "\n",
    "print(\"Conexão com banco de dados criada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular tamanhos máximos das colunas\n",
    "tamanhos_maximos = {}\n",
    "tipos_de_dados = {}\n",
    "\n",
    "for nome_collection, df in dataframes.items():\n",
    "    # Calcula tamanho máximo de cada coluna\n",
    "    tamanhos = df.map(lambda x: len(str(x))).max()\n",
    "    tamanhos_maximos[nome_collection] = tamanhos\n",
    "    \n",
    "    # Define tipo VARCHAR com o tamanho calculado\n",
    "    tipos = {coluna: VARCHAR(tamanho) for coluna, tamanho in tamanhos.items()}\n",
    "    tipos_de_dados[nome_collection] = tipos\n",
    "\n",
    "print(\"Tipos de dados calculados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Importação para SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar cada DataFrame para o SQL Server\n",
    "for nome_collection, df in dataframes.items():\n",
    "    print(f\"\\nImportando {nome_collection}...\")\n",
    "    \n",
    "    # Converter colunas que contenham listas/dicionários em JSON (pyodbc/SQLAlchemy não aceita lista/dict direto)\n",
    "    cols_to_convert = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, (list, dict))).any()]\n",
    "    for col in cols_to_convert:\n",
    "        df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, (list, dict)) else x)\n",
    "    \n",
    "    # Recalcular tamanhos máximos e tipos para este DataFrame (pode ter mudado ao serializar JSON)\n",
    "    # usamos .astype(str) para garantir que todos os valores sejam strings ao medir o tamanho\n",
    "    tamanhos = df.astype(str).apply(lambda col: col.map(len).max())\n",
    "    tipos_local = {col: VARCHAR(int(tamanho)) for col, tamanho in tamanhos.items()}\n",
    "    tipos_de_dados[nome_collection] = tipos_local  # atualizar o mapa global se quiser\n",
    "    \n",
    "    df.to_sql(\n",
    "        nome_collection,\n",
    "        engine,\n",
    "        index=False,\n",
    "        if_exists='replace',  # Mude para 'replace' se quiser substituir a tabela\n",
    "        dtype=tipos_local\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ {nome_collection} importado com sucesso! ({len(df)} registros)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IMPORTAÇÃO CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
